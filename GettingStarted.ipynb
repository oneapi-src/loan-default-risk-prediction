{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2af0007",
   "metadata": {},
   "source": [
    "## Reference Implementation\n",
    "\n",
    "### E2E Architecture\n",
    "\n",
    "![use_case_flow](assets/e2e-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89cbc5c",
   "metadata": {},
   "source": [
    "### Solution setup\n",
    "Use the following cell to change to the correct kernel. Then check that you are in the `stock` kernel. If not, navigate to `Kernel > Change kernel > Python [conda env:stock]`. Note that the cell will remain with * but you can continue running the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5e7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-defaultrisk_stock-py'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c649ea71",
   "metadata": {},
   "source": [
    "We can view a few samples of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/credit_risk_dataset.csv\")\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca3b94",
   "metadata": {},
   "source": [
    "\n",
    "For demonstrative purposes we make 2 modifications to the original dataset before experimentation using the the [`data/prepare_data.py`](data/prepare_data.py) script\n",
    "\n",
    "1. Adding a synthetic bias_variable\n",
    "    \n",
    "    For the purpose of demonstrating fairness in an ML model later, we will add a bias value for each loan default prediction. This value will be generated randomly using a simple binary probability distribution as follows:\n",
    "    ```\n",
    "\n",
    "    If the loan is defaulted i.e. prediction class 1:\n",
    "      assign bias_variable = 0 or 1 with the probability of 0 being 0.65\n",
    "\n",
    "    if the loan is not defaulted i.e. prediction class 0:\n",
    "      assign bias_variable = 0 or 1 with the probability of 0 being 0.35\n",
    "      \n",
    "    ```\n",
    "    |**Feature** | **Description** |\n",
    "    | :---: | :---: |\n",
    "    | bias_variable | synthetic biased variable |\n",
    "\n",
    "    For fairness quantification, we will define that this variable should belong to a [protected class](https://en.wikipedia.org/wiki/Fairness_(machine_learning)) and `bias_variable = 1` is the privileged group.\n",
    "\n",
    "    This variable is NOT used to train the model as the expectation is that it should not be used to make decisions for fairness purposes.\n",
    "\n",
    "2.  Splitting the dataset into 1 initial batch for training the model from scratch, and 3 additional equally sized batches for incrementally updating the trained model \n",
    "    \n",
    "    To simulate the process of incremental learning, where the model is updated on new datasets, the original training set is split into 1 batch for initially training the model from scratch, and then 3 more equally sized batches for incrementally learning.  When running incremental learning, we will be using each batch to represent a new dataset that will be used to update the model..   \n",
    "\n",
    "The final process for splitting this dataset is first, 70% for training and 30% for holdout testing.  Following this, the 70% is split as described above into 1 batch for initial training and 3 for incremental training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ed08c",
   "metadata": {},
   "source": [
    "We will now run the dataprocessing discribed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data && python prepare_data.py --num_batches 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f324b15c",
   "metadata": {},
   "source": [
    "### Model Building Process\n",
    "\n",
    "The `run_training.py` script *reads the data*, *trains a preprocessor*, and *trains an XGBoost Classifier*, and *saves the model* which can be used for future inference.\n",
    "\n",
    "The script takes the following arguments:\n",
    "\n",
    "```shell\n",
    "usage: run_training.py [-h] [--intel] [--num_cpu NUM_CPU] [--size SIZE][--trained_model TRAINED_MODEL] [--save_model_path SAVE_MODEL_PATH] --train_file TRAIN_FILE --test_file TEST_FILE\n",
    "                       [--logfile LOGFILE] [--estimators ESTIMATORS]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --intel               use intel daal4py for model optimization\n",
    "  --num_cpu NUM_CPU     number of cpu cores to use\n",
    "  --size SIZE           number of data entries to duplicate data for training and benchmarking. -1 uses the original data size. Default is -1.\n",
    "  --trained_model TRAINED_MODEL\n",
    "                        saved trained model to incrementally update. If not provided, trains a new model.\n",
    "  --save_model_path SAVE_MODEL_PATH\n",
    "                        path to save a trained model. If not provided, does not save.\n",
    "  --train_file TRAIN_FILE\n",
    "                        data file for training\n",
    "  --test_file TEST_FILE\n",
    "                        data file for testing\n",
    "  --logfile LOGFILE     log file to output benchmarking results to.\n",
    "  --estimators ESTIMATORS\n",
    "                        number of estimators to use.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c4783",
   "metadata": {},
   "source": [
    "#### Training the Initial Model\n",
    "\n",
    "Assuming the structure is set up, we can use this script with the following command to generate and save a brand new trained XGBoost Classifier ready to be used for inference.\n",
    "\n",
    "```shell\n",
    "cd src\n",
    "conda activate defaultrisk_stock\n",
    "python run_training.py --train_file ../data/batches/credit_risk_train_1.csv --test_file ../data/credit_risk_test.csv --save_model_path ../saved_models/stock/model_1.pkl\n",
    "```\n",
    "\n",
    "The output of this script is a saved model `../saved_models/stock/model_1.pkl`.  In addition, the fairness metrics on a holdout test will also be shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python run_training.py --train_file ../data/batches/credit_risk_train_1.csv --test_file ../data/credit_risk_test.csv --save_model_path ../saved_models/stock/model_1.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fff7f95",
   "metadata": {},
   "source": [
    "For the `bias_variable` generative process described above, we can see that certain values strongly deviate from 1, indicating that the model may have detected some bias and does not seem to be making equitable predictions between the two groups.  \n",
    "\n",
    "In comparison, we can adjust the generative process so that the `bias_variable` is explicitly fair independent of the outcome:\n",
    "\n",
    "```\n",
    "\n",
    "    If the loan is defaulted i.e. prediction class 1:\n",
    "      assign bias_variable = 0 or 1 with the probability of 0 being 0.5\n",
    "\n",
    "    if the loan is not defaulted i.e. prediction class 0:\n",
    "      assign bias_variable = 0 or 1 with the probability of 0 being 0.5\n",
    "      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a2ea5",
   "metadata": {},
   "source": [
    "We can do this by running our data preparation and training scripts again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data && python prepare_data.py --bias_prob=0.5 --num_batches 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python run_training.py --train_file ../data/batches/credit_risk_train_1.csv --test_file ../data/credit_risk_test.csv --save_model_path ../saved_models/stock/model_1.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb56dd2",
   "metadata": {},
   "source": [
    "indicating that the model is not biased along this protected variable.\n",
    "\n",
    "A thorough investigation of fairness and mitigation of bias is a complex process that *may require multiple iterations of training and retraining the model*, potentially excluding some variables, reweighting samples, and investigation into sources of potential sampling bias.  A few further resources on fairness for ML models, as well as techniques for mitigation include [this guide](https://afraenkel.github.io/fairness-book/intro.html) and [the `shap` package](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Explaining%20quantitative%20measures%20of%20fairness.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdf32c7",
   "metadata": {},
   "source": [
    "#### Updating the Initial Model with New Data (Incremental Learning)\n",
    "\n",
    "The same script can be used to update the trained XGBoost Classifier with new data.  We can pass in the previously trained model file from above (`../saved_models/stock/model_1.pkl`) and a new dataset file(`../data/batches/credit_risk_train_2.csv`) in the same format as the original dataset to process an incremental update to the existing model and output a new model.  \n",
    "\n",
    "```shell\n",
    "cd src\n",
    "conda activate defaultrisk_stock\n",
    "python run_training.py --train_file ../data/batches/credit_risk_train_2.csv --test_file ../data/credit_risk_test.csv --trained_model ../saved_models/stock/model_1.pkl --save_model_path ../saved_models/stock/model_2.pkl\n",
    "```\n",
    "\n",
    "The output of this script is a newly saved model `../saved_models/stock/model_2.pkl` as well as new fairness metrics/plots on this model.  This new model can be deployed in the same environment as before and will use this newly updated model.\n",
    "\n",
    "***The accuracy of this model, trained on the original dataset as described in the instructions above, on a holdout test set reachs ~90% with an AUC of ~0.87.  Incremental updates for this particular dataset maintains the accuracy of this model on a holdout test set at ~90% with an AUC of ~0.87.  This indicates that the model has saturated and that the data is not changing over time either.***\n",
    "\n",
    "> **Implementation Note:** For an XGBoost Classifier, updating the model using the XGBoost built in functionality simply adds additional boosting rounds/estimators to the model, constructed using only the new data.  This does **not** update existing estimators.  As a result, after every incremental round, the model grows more complex while remembering old estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bbb496",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python run_training.py --train_file ../data/batches/credit_risk_train_2.csv --test_file ../data/credit_risk_test.csv --trained_model ../saved_models/stock/model_1.pkl --save_model_path ../saved_models/stock/model_2.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302db2a1",
   "metadata": {},
   "source": [
    "### Model Inference\n",
    "\n",
    "The saved model from each model iteration can be used on new data with the same features to infer/predict the probability of a default.  This can be deployed in any number of ways.  When the model is updated on new data, the deployed model can be transitioned over to the new model to make updated inferences given that performance is better and that the model meets the standards of the organization at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335b423",
   "metadata": {},
   "source": [
    "### Running Inference\n",
    "\n",
    "To use this model to make predictions on new data, we can use the `run_inference.py` script which takes in a saved model and a dataset to predict on, outputting a json to console with the above format.\n",
    "\n",
    "The run_inference script takes the following arguments:\n",
    "\n",
    "```shell\n",
    "usage: run_inference.py [-h] [--is_daal_model] [--silent] [--size SIZE]\n",
    "                        [--trained_model TRAINED_MODEL] --input_file\n",
    "                        INPUT_FILE [--logfile LOGFILE]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --is_daal_model       toggle if file is daal4py optimized\n",
    "  --silent              don't print predictions. used for benchmarking.\n",
    "  --size SIZE           number of data entries for eval, used for\n",
    "                        benchmarking. -1 is default.\n",
    "  --trained_model TRAINED_MODEL\n",
    "                        Saved trained model to incrementally update. If None,\n",
    "                        trains a new model.\n",
    "  --input_file INPUT_FILE\n",
    "                        input file for inference\n",
    "  --logfile LOGFILE     Log file to output benchmarking results to.\n",
    "```\n",
    "\n",
    "To run inference on a new data file using one of the saved models, included by the above data preparation as 30% of the full training set, `../data/credit_risk_test.csv` we can run the command:\n",
    "\n",
    "```shell\n",
    "cd src\n",
    "conda activate defaultrisk_stock\n",
    "python run_inference.py --trained_model ../saved_models/stock/model_1.pkl --input_file ../data/credit_risk_test.csv\n",
    "```\n",
    "\n",
    "which outputs a json representation of the predicted probability of default for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c5977",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python run_inference.py --trained_model ../saved_models/stock/model_1.pkl --input_file ../data/credit_risk_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70942e3d",
   "metadata": {},
   "source": [
    "Running inference on an incrementally updated model can be done using the same script, only specifying the updated model:\n",
    "\n",
    "```shell\n",
    "cd src\n",
    "conda activate defaultrisk_stock\n",
    "python run_inference.py --trained_model ../saved_models/stock/model_2.pkl --input_file ../data/credit_risk_test.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python run_inference.py --trained_model ../saved_models/stock/model_2.pkl --input_file ../data/credit_risk_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057162ed",
   "metadata": {},
   "source": [
    "## Optimizing the E2E Reference Solution with Intel® oneAPI\n",
    "\n",
    "### Optimized E2E Architecture with Intel® oneAPI Components\n",
    "\n",
    "![Use_case_flow](assets/e2e-workflow-optimized.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82e524",
   "metadata": {},
   "source": [
    "### Optimized Reference Solution Implementation \n",
    "\n",
    "#### Model Building Process with Intel® Optimizations\n",
    "\n",
    "As Intel® optimizations are directly enabled by using XGBoost >v0.81 and the environment setup for the optimized version installs XGBoost v1.4.2, the `run_training.py` script can be run with no code changes otherwise to obtain a saved model with XGBoost v1.4.2. The same training process can be run, optimized with Intel® oneAPI as follows:\n",
    "\n",
    "```shell\n",
    "cd src\n",
    "conda activate defaultrisk_intel\n",
    "python run_training.py --train_file ../data/batches/credit_risk_train_1.csv --test_file ../data/credit_risk_test.csv --save_model_path ../saved_models/stock/model_1.pkl\n",
    "```\n",
    "\n",
    "By toggling the `--intel` flag, the same process can also be used to save a **oneDAL optimized model**.  For example, the following command creates 2 saved models:\n",
    "\n",
    "```shell\n",
    "cd src\n",
    "conda activate defaultrisk_intel\n",
    "python run_training.py --train_file ../data/batches/credit_risk_train_1.csv --test_file ../data/credit_risk_test.csv --save_model_path ../saved_models/intel/model_1.pkl --intel\n",
    "```\n",
    "\n",
    "1. ../saved_models/intel/model_1.pkl \n",
    "    \n",
    "    A saved XGBoost v1.4.2 model \n",
    "\n",
    "2. ../saved_models/stock/model_1_daal.pkl\n",
    "\n",
    "    The same model as above, but optimized using oneDAL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2deab5c",
   "metadata": {},
   "source": [
    "**change kernel to Python[conda env:defaultrisk_intel]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c550cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-defaultrisk_intel-py'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e81db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python run_training.py --train_file ../data/batches/credit_risk_train_1.csv --test_file ../data/credit_risk_test.csv --save_model_path ../saved_models/stock/model_1.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c188035",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python run_training.py --train_file ../data/batches/credit_risk_train_1.csv --test_file ../data/credit_risk_test.csv --save_model_path ../saved_models/intel/model_1.pkl --intel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bdddbe",
   "metadata": {},
   "source": [
    "#### Model Inference with Intel® Optimizations\n",
    "\n",
    "Inference with Intel® optimizations for v1.4.2 can also be enabled simply by using XGBoost >v0.81 as mentioned above.  To run inference on the v1.4.2 model, we can use the same `run_inference.py` script with no modifications to the call, passing in the desired v1.4.2 model:\n",
    "\n",
    "```shell\n",
    "cd src\n",
    "conda activate defaultrisk_intel\n",
    "python run_inference.py --trained_model ../saved_models/intel/model_1.pkl --input_file ../data/credit_risk_test.csv\n",
    "```\n",
    "\n",
    "To run inference on an Intel® oneDAL optimized model, the same `run_inference.py` script can be used, but the passed in model needs to be the saved daal4py version from training, and the `--is_daal_model` flag should be toggled:\n",
    "\n",
    "```shell\n",
    "cd src\n",
    "conda activate defaultrisk_intel\n",
    "python run_inference.py --trained_model ../saved_models/intel/model_1_daal.pkl --input_file ../data/credit_risk_test.csv --is_daal_model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2914378",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python run_inference.py --trained_model ../saved_models/intel/model_1.pkl --input_file ../data/credit_risk_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd68c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python run_inference.py --trained_model ../saved_models/intel/model_1_daal.pkl --input_file ../data/credit_risk_test.csv --is_daal_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea01140",
   "metadata": {},
   "source": [
    "## Performance Observations\n",
    "\n",
    "In the following, we perform benchmarks comparing the Intel® technologies vs the stock alternative measuring the following tasks:\n",
    "\n",
    "### ***1. Benchmarking Incremental Training with Intel® oneAPI Optimizations for XGBoost***\n",
    "\n",
    "Training is conducted using Intel® oneAPI XGBoost v.1.4.2.  This is more efficient for larger datasets and model complexity.  The same optimizations apply when incrementally updating an existing model with new data.  For XGBoost, as incremental learning naturally increases the complexity of the model, later iterations may benefit more strongly from Intel® optimizations. \n",
    "\n",
    "As fairness and bias can be a major component in deploying a model for default risk prediction, in order to mitigate detected bias, many techniques must be explored such as dropping columns and rows, reweighting, resampling, and collecting new data.  Each of these new techniques requires a new model to be trained/incrementally updated, allowing for Intel® optimizations to continuously accelerate the discovery and training process beyond a single training iteration.\n",
    "\n",
    "### ***2. Batch Inference with Intel® oneAPI Optimizations for XGBoost and Intel® oneDAL***\n",
    "\n",
    "Once a model is trained, it can be deployed for inference on large data loads to predict the default risk across many different clients and many different potential loan attributes.  For other realistic scenarios, this can be used across a lot of different term structures and for scenario testing and evaluation.  \n",
    "\n",
    "We benchmark batch inference using an v0.81 XGBoost model, a v1.4.2 XGBoost model, and a v1.4.2 XGBoost model optimized with Intel® oneDAL.\n",
    "\n",
    "### Training Experiments\n",
    "\n",
    "To explore performance across different dataset sizes, we replicate the original dataset to a larger size and add noise to ensure that no two data points are exactly the same.  Then we perform training and inference tasks on the following experimental configurations:\n",
    "\n",
    "  **Experiment:**\n",
    "    Model is initially trained on 3M data points.  Following this, the model is *incrementally updated* using 1M data points and used for **inference** on 1M data points.  This *incremental update* and *inference* process is repeated for 3 update rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7ec657",
   "metadata": {},
   "source": [
    "We will now run the training benchmarks, we start by changing to our stock environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b583c28",
   "metadata": {},
   "source": [
    "We will now run the training benchmarks, we start by changing to our stock environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f44a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-defaultrisk_stock-py'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d5db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list | grep xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && bash benchmark_incremental_training_stock.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a434e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && bash benchmark_inference_stock.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4f12f",
   "metadata": {},
   "source": [
    "We now change to our intel environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f7d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-defaultrisk_intel-py'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list | grep xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc28a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && bash benchmark_incremental_training_intel.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b2970",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && bash benchmark_inference_intel.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a822843c",
   "metadata": {},
   "source": [
    "Now we can create tables and graphs to ilustrate the performance benefits in training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08691a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.getenv('workdir'))\n",
    "\n",
    "from notebooks.utils import benchmarking_utils\n",
    "benchmarking_utils.print_training_benchmark_bargraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.getenv('workdir'))\n",
    "\n",
    "from notebooks.utils import benchmarking_utils\n",
    "benchmarking_utils.print_inference_benchmark_bargraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ab2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:stock]",
   "language": "python",
   "name": "conda-env-stock-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "47ddfd05091c518f1457bda86c2e52f2367741cdbeac658212cf94c78cf8a125"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
